# 손실 줄이기
### Overview
- 손실함수는 y=x^2꼴
- 손실함수의 경사가 작아지는 방향으로 다음 모델을 업데이트하는 과정을 반복
- 경사가 낮아지는 방향으로 지나면 극소값을 지남
- 얼마나 많이 경사가 낮아지는 방향으로 이동해야 하는가?
  + 학습률이 크면 많이 이동
  + 학습률이 작으면 적게 이동
- 초기값이 중요한 이유
  + 극소값이 하나면 좋은데 대부분의 경우 극소값이 하나가 아님
  + 특히 딥러닝의 경우에는 계란판 모양임

### 반복방식
- `y = b + w1x1`
- L2 손실[= (y-y') 제곱의 평균]을 최소화하는 방향으로 이동하도록 b, w1를 업데이트한다.
- 이 때 L2를 계속 구하면서 전체 손실이 변하지 않거나 매우 느리게 변할 때까지 반복하면 되는데, 이 경우를 모델이 수렴했다고 한다.

### 경사하강법
- 손실함수를 최소화하는 w_i를 찾도록 하는 방법
- 시작점(w,b)에서의 미분값(기울기)을 구하여 그 반대방향으로 이동함
  + 기울기(+) => 그 반대방향(-)으로 이동
  + 기울기(-) => 그 반대방향(+)으로 이동
### 학습률
- 경사하강법에서 다음 지점을 결정하는 방법
  + 다음지점 = 현재지점 + (-)x기울기x학습률
- 학습률(=보폭)
  + 너무 작으면 학습시간이 너무 오래 걸림
  + 너무 크면 최저점을 못찾을 수도 있음
- 골디락스 학습률
  + 골디락스: 너무 크지도 너무 작지도 않은 적절한 상태
  + 골디락스 학습률을 구하기 위해서 손실함수의 기울기가 크면 큰 학습률을, 손실함수의 기울기가 작으면 작은 학습률을 설정

### 학습률 최적화
- 실습 1. 학습률 0.1인 경우 손실 최저점에 도달할때까지 몇단계?
  + 0.1: 81단계
- 실습 2. 학습률 1인 경우 손실 최저점에 도달할때까지 몇단계?
  + 1.0: 6단계
- 실습 3. 학습률 4인 경우 손실 최저점에 도달할때까지 몇단계?
  + 발산한다
  + 경사하강법으로 손실 최저점에 도달할 수 없습니다. 학습률을 바꿔서 다시 시도해 보세요.
- 선택과제: 경사하강법에서 최저점에 도달하는 단계수를 최소화하는 골디락스 학습률?
  + 0.5: 14회
  + 0.6: 12회
  + 0.7: 10회
  + 0.8: 8회
  + 0.9: 7회
  + 1.0: 6회
  + 1.1: 5회
  + 1.2: 4회
  + 1.3: 4회
  + 1.4: 3회
  + 1.5: 2회
  + 1.6: 1회 => `골디락스 학습률!`
  + 1.7: 2회
  + 1.8: 3회
  + 1.9: 3회

### 확률적 경사하강법
- 배치: 한 반복에서 기울기를 계산하는데 사용하는 예의 총 개수
- 반복이 여러번 되는데 매번 기울기 계산을 위해 무작위로 선택된 적은 데이터셋으로 기울기 계산
- 확률적 경사하강법(SGD)
  + 반복당 하나의 예(배치크기 1)만을 사용
  + 반복이 여러번일 경우 효과 있음
  + 노이즈가 심하게 나타남
- 미니 배치 확률적 경사하강법(미니 배치 SGD)
  + 전체 배치 반복과 SGD 간의 정충안
  + 무작위로 10개~1000개 사이의 예로 구성
  + SGD보다 노이즈 적고, 전체 배치보다 효율적

## 플레이그라운드 실습
- 점
  + 파란색 점: 스팸메일x
  + 주황색 점: 스팸메일o
- 배경
  + 모델이 예측하는 라벨
  + 같은 점에 같은 배경이 있으면 올바른 것
  + 배경색이 진할수록 예측 확률이 더 높음

### 작업 1. (높은) 학습률 실습
- 학습률 3: 학습 손실 = 0.3정도
### 작업 2. (낮은) 학습률 실습
- 학습률 1: 학습 손실 = 0.2 정도
- 학습률 0.3: 학습 손실 = 0.18 정도
- 학습률 0.1: 학습 손실 = 0.2 정도

## 이해도 확인: 배치크기
다음 중 대량의 데이터 세트에서 경사하강법을 수행할 때 더 효율적인 배치 크기는 어느 것일까요?
- 소규모 배치 또는 예가 하나뿐인 배치(SGD)
  + 데이터세트가 대량이라 중복이 많고 시간이 오래 걸린다.
  + 소규모 배치로도 정확도는 비슷하고 효율적인 기울기 계산이 가능하다.
