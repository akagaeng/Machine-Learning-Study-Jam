# 신경망 소개

## 비선형 분류 문제
- 비선형이라는 의미는 `b + w1x1 + w2x2`와 같은 형태의 모델로써 라벨을 정확하게 예측할 수 없다는 의미
- 즉, '결정표면'이 선이 아니다.

### 비선형 문제 해결을 위한 방안
- 히든 레이어
  + 선형 모델에 히든 레이어 1개 추가하더라도 선형
  + 선형 모델에 히든 레이어 2개 추가하더라도 선형
  + ...
- 활성화 함수
  + 각 히든 레이어의 노드가 비선형 함수(활성화 함수)를 통과하도록 하여 비선형 문제 모델림
  + 입력 - 히든 레이어1 - 비선형 변환 레이어(활성화 함수) - 히든 레이어2 - 출력
- 활성화 함수와 히든 레이어
  + 활성화 함수가 추가되면 히든 레이어 추가시마다 비선형성이 누적됨
  + 입출력 간의 매우 복잡한 관계를 모델링할 수 있다
  
### 일반적인 활성화 함수

시그모이드 활성화 함수
 
```
F(x) = 1 / (1 + e^(-x))
```

정류 선형 유닛(ReLU) 활성화 함수

```
F(x) = max(0, x)
```

시작은 ReLU로!

## 신경망 소개: Playground 실습

### 처음 만들어보는 신경망
> 신경망을 통해 명시적인 특성 교차를 사용하지 않고도 비선형 모델을 학습할 수 있다.

**과제 1**: 주어진 모델은 두 개의 입력 특성을 하나의 뉴런으로 결합합니다. 이 모델이 비선형성을 학습할 수 있을까요? 모델을 실행하여 추측을 확인해 보세요.
- 활성화가 `선형`으로 되어 있어 어떠한 비선형성도 학습할 수 없고, 손실도 거의 0.5로 매우 높다.

**과제 2**: 히든 레이어의 뉴런 수를 1개에서 2개로 늘려보고 선형 활성화에서 ReLU와 같은 비선형 활성화로 변경해 보세요. 비선형성을 학습하는 모델을 만들 수 있나요?
- 2개인 경우에도 어느 정도는 가능하나, 2개보다는 2개 이상인 경우 ReLU, Tanh, 시그모이드 활성화 함수로 비선형성을 잘 학습합니다. 

**과제 3**: 히든 레이어 및 레이어당 뉴런을 추가하거나 삭제하여 실험을 계속해 봅니다. 또한 자유롭게 학습률, 정규화 및 기타 학습 설정을 변경해 보세요. 테스트 손실을 0.177 이하로 얻는 데 사용할 수 있는 가장 적은 노드 및 레이어 수는 얼마인가요?
- 레이어1: 4개, 레이어2: 3개, 레이어3: 2개
- 학습률: 0.01
- 활성화: ReLU
- 정규화율: 0.003

위 케이스에서 테스트 손실 0.171, 학습 손실 0.132

### 신경망 초기화

> 이 실습에서는 다시 XOR 데이터를 사용하는데, 학습용 신경망의 반복성과 초기화의 중요성을 살펴봅니다.

**과제 1**: 주어진 모델을 4~5회 실행합니다. 매번 시도하기 전에 네트워크 초기화 버튼을 눌러 임의로 새롭게 초기화합니다. 수렴을 보장하기 위해 매 시도마다 최소 500단계를 실행하도록 합니다. 각 모델 출력이 어떤 형태로 수렴하나요? 이 결과가 비볼록 최적화에서 초기화의 역할에 어떤 의미를 가지나요?

#### 시도
- 시도 1: 테스트 손실 0.35 정확히 수렴x
- 시도 2: 테스트 손실 0.17 정확히 수렴o
- 시도 3: 테스트 손실 0.17 정확히 수렴o
- 시도 4: 테스트 손실 0.34 정확히 수렴x
- 시도 5: 테스트 손실 0.19 정확히 수렴o

#### 결과
- 초기화 상태에 따라 모델이 정확히 수렴하기도 하고, 그렇지 않기도 했다. 
- 손실도 시도마다 차이가 났다.

**과제 2**: 레이어 한 개와 추가 노드를 몇 개 더 추가하여 모델을 약간 더 복잡하게 만들어 보세요. 과제 1의 시도를 반복합니다. 결과에 안정성이 보강되나요?

#### 시도
- 시도 1: 테스트 손실 0.18 정확히 수렴o
- 시도 2: 테스트 손실 0.19 정확히 수렴o
- 시도 3: 테스트 손실 0.17 정확히 수렴o
- 시도 4: 테스트 손실 0.17 정확히 수렴o
- 시도 5: 테스트 손실 0.17 정확히 수렴o

#### 결과
- 초기화 상태와 무관하게 모델이 유사한 모양으로 수렴하였다. 
- 손실도 시도마다 비슷했다.

### 나선형 신경망

**과제 1**: X1과 X2만 사용하여 가능한 한 최고의 모델로 학습시켜 봅니다. 자유롭게 레이어와 뉴런을 추가 또는 삭제하고 학습률, 정규화율, 배치 크기와 같은 학습 설정을 변경해 보세요. 얻을 수 있는 최고의 테스트 손실은 얼마인가요? 모델 출력 표면은 얼마나 매끄러운가요?
- 4개 히든 레이어, 5개, 4개, 4개, 4개 노드로도 나선형 학습 제대로 되지 않음

**과제 2**: 신경망이라도 최고의 성능을 달성하기 위해서는 특성 추출이 일부 필요합니다. 추가 교차 특성이나 sin(X1), sin(X2)과 같은 기타 변환을 추가해 보세요. 더 나은 모델이 도출되나요? 모델 표면이 더 매끄러워지나요?
- 복잡한 모델을 추가한 경우 히든 레이어 수가 적어도 손실이 낮아지고, 수렴 속도도 빨라짐
- 그러나 모든 경우에 그런 것은 아니고, 정규화, 학습률, 정규화율 등 많은 변수들과 다 상관이 있을 수 있음 



