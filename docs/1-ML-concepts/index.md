# 1. ML 개념

## ML 소개
여기서 배운 내용을 활용하면 SW 엔지니어로서 세가지(+a)를 더 잘 할 수 있다.
1. 프로그래밍 시간을 줄일 수 있는 도구를 얻게 된다. 
  -  ex: 맞춤법 오류 수정하는 프로그램 작성시 사람vs머신러닝
2. 제품을 맞춤설정하여 특정 집단의 사용자에게 더 잘 맞는 제품을 제공할 수있다. 
  - ex: 철자 교정 도구를 100개 언어로 제작시 사람vs머신러닝
3. 프로그래머로서 수동으로 할 방법이 없어 보이는 문제를 해결할 수 있다. 
  - 알고리즘에게 무엇을 하라고 명령할 필요 없이 수많은 예를 보여주면 문제가 해결된다.
4. +a (철학적인 이유)
  - 문제에 관해 생각하는 방법을 바꿈
  - 논리적/수학적 사고 -> 불확실한 세계 관찰/통계적 사고
  - 문제에 대한 시야가 넓어지고 이전에 가지 못한 새 영역으로 나갈 수 있다.

## ML 문제로 표현하기

### 동영상
Supervising machine Framework
-  라벨(Label)
    + 지도 학습에서 학습을 위해 제공하는 것
    + 이메일 스팸 필터링이라면 label = {'spam', 'not spam'} 정도가 될 것
    + 예측하는 실제 항목으로써 선형회귀에서 `y` 변수
- 특성(feature)
    + 데이터를 설명하는 입력변수 `x = {x_1, x_2, ...}`
    + 이메일 스팸 필터링이라면 이메일에 포함된 단더, 이메일주소, 라우팅 정보, 헤더 정보 ... 가 될 수 있다.
- 모델
    + 예측을 하도록 해줌
    + (특성o, 라벨x) => 모델 => **라벨 예측(=추론)!**

### ML 용어

머신러닝이란?
- 입력을 결합하여 이전에 본 적이 없는 데이터를 적절히 예측하는 방법을 학습한다.

라벨
- 예측하는 항목
- 단순 선형회귀의 `y` 변수)

특성
- 입력변수
- 단순 선형회귀의 `x` 변수
- 특성은 1개 이상 사용하며, 복잡한 프로젝트일수록 많이 사용

예
- 데이터(x)의 인스턴스
- 라벨이 있는 예(특성o, 라벨o): 학습용
- 라벨이 있는 예(특성o, 라벨x): 예측용

모델
- 특성과 라벨의 관계를 정의한다.
- 학습
  + 모델을 만들거나 배우는 것
  + (특성o, 라벨o) 데이터로 학습시킴
- 추론
  + 학습된 모델을 라벨이 없는 예에 적용하는 것
  + (특성o, 라벨x) 데이터 => 모델 => 추론!

회귀와 분류
- 회귀 모델
  + 연속적인 값을 예측 (수치, 확률, ...)
- 분류 모델
  + 불연속적인 값을 예측 (스팸인지아닌지, 고양이/강아지 판단, ...)

### 이해도 확인
> 주어진 이메일이 '스팸' 인지 '스팸이 아닌지' 예측하기 위해 지도 머신러닝 모델을 개발한다고 합시다. 다음 중 참인 내용은 무엇일까요?
- 일부 라벨은 신뢰할 수 없을 수도 있습니다.(o)
- '스팸' 또는 '스팸 아님'으로 표시되지 않은 이메일은 라벨이 없는 예입니다. (o)
- 모델을 학습시키는 데는 라벨이 없는 예를 사용합니다.(x)
  + 라벨 있는 예로 학습시킨다.
- 제목 헤더의 단어는 라벨로 사용하기에 적절합니다.(x)
  + 특성: 제목 헤더의 단어, ...
  + 라벨: 스팸o/스팸x

> 한 온라인 신발가게에서 사용자에게 맞춤형 신발을 추천하는 지도 ML 모델을 만들려고 합니다. 즉 이 모델에서는 철수에게 특정 신발을 추천하고 영희에게는 다른 신발을 추천합니다. 다음 중 참인 내용은 무엇일까요?
- 사용자의 신발 설명 클릭수는 유용한 라벨입니다. (o)
  + 마음에 드는 것만 클릭해서 설명을 볼 것이므로
- 사용자가 아주 좋아하는 신발은 유용한 라벨입니다. (x)
  + 좋아한다는 것은 추상적임 => 관찰 불가능/수량화 불가
  + 추상적인 것을 관찰가능한 다른 측정학목으로 변경하여 라벨화
- 신발 크기는 유용한 특성입니다. (o)
  + 마음에 드는 신발인지 판단하는 기준이 된다.
- 신발의 아름다움은 유용한 특성입니다. (x)
  + 모호한 개념
  + 구체적이고 수량화 가능한 특성이 좋은 특성

## ML로 전환하기
### 선형회귀
```
y' = wx + b     : 1차원의 경우(1가지 특성에 의존)
y' = w_1x_1 + b + w_2x_2 + ... : 차원이 늘어나는 경우(여러가지 특성)
```
- y': 예측된 라벨(얻고자 하는 결과)
- b : 편향(y절편), w_0라고 하기도 한다.
- w_1: 특성1의 가중치
- x_1: 특성1(알려진 입력)

> 알려진 특성 입력하여 모델 만들고,<br>
> 알려지지 않은 새로운 특성에서 라벨 추론

### 학습 및 손실
> 모델을 학습시킨다 = 라벨이 있는 데이터로부터 올바를 가중치(w)와 편향값(b)을 결정한다.

> 경험적 위험 최소화: 손실을 최소화하는 모델을 찾아봄으로써 모델을 만들어 낸다.

즉, 손실이 가장 작은 모델이 가장 좋은 모델이다!

#### 제곱손실(L2 손실)
```
= (observation - prediction(x))^2
= (y-y')^2
```
- 참고로 L1 손실은 `|  y - y' |`

#### 평균제곱오차(MSE, Mean Squared Error)
- L2 손실을 평균낸 것
```
MSE = 1/N * sum( (y-prediction(x))^2 )
```

### 손실 줄이기
#### Overview
- 손실함수는 y=x^2꼴
- 손실함수의 경사가 작아지는 방향으로 다음 모델을 업데이트하는 과정을 반복
- 경사가 낮아지는 방향으로 지나면 극소값을 지남
- 얼마나 많이 경사가 낮아지는 방향으로 이동해야 하는가?
  + 학습률이 크면 많이 이동
  + 학습률이 작으면 적게 이동
- 초기값이 중요한 이유
  + 극소값이 하나면 좋은데 대부분의 경우 극소값이 하나가 아님
  + 특히 딥러닝의 경우에는 계란판 모양임

#### 반복방식
- `y = b + w1x1`
- L2 손실[= (y-y') 제곱의 평균]을 최소화하는 방향으로 이동하도록 b, w1를 업데이트한다.
- 이 때 L2를 계속 구하면서 전체 손실이 변하지 않거나 매우 느리게 변할 때까지 반복하면 되는데, 이 경우를 모델이 수렴했다고 한다.

### 경사하강법
- 손실함수를 최소화하는 w_i를 찾도록 하는 방법
- 시작점(w,b)에서의 미분값(기울기)을 구하여 그 반대방향으로 이동함
  + 기울기(+) => 그 반대방향(-)으로 이동
  + 기울기(-) => 그 반대방향(+)으로 이동
### 학습률
- 경사하강법에서 다음 지점을 결정하는 방법
  + 다음지점 = 현재지점 + (-)x기울기x학습률
- 학습률(=보폭)
  + 너무 작으면 학습시간이 너무 오래 걸림
  + 너무 크면 최저점을 못찾을 수도 있음
- 골디락스 학습률
  + 골디락스: 너무 크지도 너무 작지도 않은 적절한 상태
  + 골디락스 학습률을 구하기 위해서 손실함수의 기울기가 크면 큰 학습률을, 손실함수의 기울기가 작으면 작은 학습률을 설정

### 학습률 최적화
- 실습 1. 학습률 0.1인 경우 손실 최저점에 도달할때까지 몇단계?
  + 0.1: 81단계
- 실습 2. 학습률 1인 경우 손실 최저점에 도달할때까지 몇단계?
  + 1.0: 6단계
- 실습 3. 학습률 4인 경우 손실 최저점에 도달할때까지 몇단계?
  + 발산한다
  + 경사하강법으로 손실 최저점에 도달할 수 없습니다. 학습률을 바꿔서 다시 시도해 보세요.
- 선택과제: 경사하강법에서 최저점에 도달하는 단계수를 최소화하는 골디락스 학습률?
  + 0.5: 14회
  + 0.6: 12회
  + 0.7: 10회
  + 0.8: 8회
  + 0.9: 7회
  + 1.0: 6회
  + 1.1: 5회
  + 1.2: 4회
  + 1.3: 4회
  + 1.4: 3회
  + 1.5: 2회
  + 1.6: 1회 => `골디락스 학습률!`
  + 1.7: 2회
  + 1.8: 3회
  + 1.9: 3회

### 확률적 경사하강법
- 배치: 한 반복에서 기울기를 계산하는데 사용하는 예의 총 개수
- 반복이 여러번 되는데 매번 기울기 계산을 위해 무작위로 선택된 적은 데이터셋으로 기울기 계산
- 확률적 경사하강법(SGD)
  + 반복당 하나의 예(배치크기 1)만을 사용
  + 반복이 여러번일 경우 효과 있음
  + 노이즈가 심하게 나타남
- 미니 배치 확률적 경사하강법(미니 배치 SGD)
  + 전체 배치 반복과 SGD 간의 정충안
  + 무작위로 10개~1000개 사이의 예로 구성
  + SGD보다 노이즈 적고, 전체 배치보다 효율적

### 플레이그라운드 실습

