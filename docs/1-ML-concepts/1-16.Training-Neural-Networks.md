# 신경망 학습
## 신경망 학습: 권장사항

### 역전파의 실패 사례
1. 경사 소실
  - 입력 쪽에 가까운 하위 레이어의 경사가 매우 작아질 수 있다.
  - 이 경우 학습속도가 크게 저하되거나 학습이 중지된다.

> [해결책] ReLU 활성화 함수를 통해 경사 소실을 방지할 수 있다.

2. 경사 발산
  - 네트워크에 가중치가 매우 크면 경사가 너무 커져서 수렴하지 못하고 발산하는 현상이 생길 수 있다.
  - batch 정규화를 사용하여 경사발산을 방지할 수 있다.
> [해결책] 학습률을 낮춰서 경사발산을 방지할 수 있다.
3. ReLU 유닛 소멸
  - ReLU 유닛의 가중 합이 0 미만으로 떨어지면 ReLU 유닛이 고착화되어 활동이 출력되지 않는다.
  - 이 경우 네트워크의 출력에 어떠한 영향도 없고, 역전파 과정에서 경사가 더이상 통과할 수 없다.
  - 즉, 경사의 근원이 단절되어 가중 합이 다시 0 이상으로 상승할만큼 ReLU가 변화하지 못할 수 있다.

> [해결책] 학습률을 낮추어 ReLU 유닛 소멸을 방지할 수 있다.

### 드롭아웃 정규화

- 신경망에서 사용하는 정규화 유형
- 단일 경사 스텝에서 유닛 활동을 무작위로 배제하는 방식
- 드롭아웃을 반복할수록 정규화가 강력해진다.
- 정규화 정도
  + 0.0: 드롭아웃 정규화 없음
  + 1.0: 전체 드롭아웃으로써 모델에서 학습을 수행하지 않는다.


  