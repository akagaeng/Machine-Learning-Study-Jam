# 직관력 테스트
```
테스트 세트 및 학습 세트를 사용하여 모델 개발 반복 과정을 진행: 반복 과정에서는 학습 데이터를 학습하고 테스트 데이터로 평가하면서 테스트 데이터에 대한 평가 결과를 근거로 학습률, 특성 등의 다양한 모델 초매개변수를 선택하고 변화를 줍니다. 그런데 이런 절차를 많이 반복하게 되면 특정 테스트 세트의 특이성에 은연중에 적응하게 됩니다. 즉, 특정 테스트 세트로 자주 평가할수록 해당 테스트 세트에 대한 암시적 과적합이 나타날 위험이 커집니다.
```

# 다른 파티션: 추가 분할

## 데이터 세트를 2개(학습/테스트)로 분리
- 학습세트로 모델을 학습시키고, `테스트세트`로 모델을 평가
- `테스트세트`의 결과에 따라 모델을 조정
- `테스트세트`에서 가장 우수한 결과를 보이는 모델을 선택
- `테스트세트`로 결과를 확인
- 이 경우, 테스트 세트에 과적합이 될 수 있다!

## 데이터 세트를 3개(학습/검증/테스트)로 분리
- 학습세트로 모델을 학습시키고, `검증세트`로 모델을 평가
- `검증세트`의 결과에 따라 모델을 조정
- `검증세트`에서 가장 우수한 결과를 보이는 모델을 선택
- `테스트세트`로 결과를 확인
- 이 경우, 테스트세트가 적게 노출되므로 더 우수함!

# 프로그래밍 실습

## 작업 1: 데이터 조사
- 사용가능한 입력 특성 9가지
  + latitude, longitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_person
- 이상한 점
  + median_income
    * 0.5~15 사이의 값을 가진다.
    * 얼마나 큰지에 대해서 명확히 알 수 없다. 
  + median_house_value
    * 15~500 사이의 값을 가진다.
    * `'이 값은 인위적인 한도로 보입니다.'라고 되어있는데, 그것보다는 더 큰 값이 있을텐데 막아둔 것 같다는 의미인가?`
  + rooms_per_person
    * 0.0~55.2(29.4) 사이의 값을 갖는데 max와 75%(2.3)와의 차이가 매우 크다.
    * 이는 데이터 손상의 증거일 수도 있다.

## 작업 2: 위도/경도와 주택 가격 중앙값을 비교하여 도식화
- langitude와 longitude를 점을 찍고, median_house_value를 색상으로 표현
  + 주택 가격이 높은 지역이 빨간색으로 표시됨
- 실제 지도와 비교해 보면 학습 세트는 어느 정도 기대에 부합하지만 검증 세트는 그렇지 못합니다.
  + 검증세트에서는 캘리포니아나 로스앤젤레스와 같이 주택 가격이 높은 지역이 빨간색으로 표시되지 않음
- 검증 세트와 학습 세트는 분포가 대체로 비슷해야 하며, 분포가 다르면 학습세트와 검증 세트를 만드는 방법이 잘못되어있을 가능성이 높으므로 세트를 만드는 작업을 다시 해야 한다.

> 데이터 세트와 검증 세트의 분포는 대략 같아야 한다.

## 작업 3: 데이터 가져오기 및 전처리 코드로 돌아가서 버그가 있는지 확인

> ML의 디버깅은 코드 디버깅이 아닌 데이터 디버깅인 경우가 많다!

### 해결책: `설정` 부분의 코드를 아래와 같이 수정
```python
california_housing_dataframe = pd.read_csv("https://dl.google.com/mlcc/mledu-datasets/california_housing_train.csv", sep=",")

# ====== 아래 주석을 해제! ======
# california_housing_dataframe = california_housing_dataframe.reindex(
#    np.random.permutation(california_housing_dataframe.index))
```

### 결과
데이터 세트와 테스트 세트의 분포가 비슷해졌다.

## 작업 4: 모델 학습 및 평가

### 빈칸의 코드 채워넣기
```
  training_input_fn = lambda: my_input_fn(
      training_examples, 
      training_targets["median_house_value"], 
      batch_size=batch_size)
  predict_training_input_fn = lambda: my_input_fn(
      training_examples, 
      training_targets["median_house_value"], 
      num_epochs=1, 
      shuffle=False)
  predict_validation_input_fn = lambda: my_input_fn(
      validation_examples, validation_targets["median_house_value"], 
      num_epochs=1, 
      shuffle=False)

...

  training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)
  training_predictions = np.array([item['predictions'][0] for item in training_predictions])
```

### 초매개변수를 다양하게 설정하기
- learning_rate의 증가(0.00001~0.00003)
- steps의 증가(100 ~ 500)
- batch_size의 증가(1 ~ 5)

- (3E-5, 500, 5): Period 09에서 RMSE(167.50)
- (3E-5, 5000, 1): Period 09에서 RMSE
(200.57)
  + steps 커지면 발산(RMSE 증가)

참고: step, batch size, epoch
- epoch: 학습 반복 횟수(총 샘플을 몇번 학습시킬지)
- batch: 모델 학습의 반복 1회. 경사(가중치) 업데이트 1회에 사용되는 예의 집합
- batch size: 몇개의 샘플로 가중치를 갱신할 것인지. 배치 하나에 포함되는 예의 개수
- step: 배치 하나에 대한 평가
- step size: 학습률의 동의어(learning rate)

## 작업 5: 테스트 데이터로 평가
```
training RMSE보다 검증(validation) RMSE가 높게 나왔는데, 이에 비해서 테스트 세트의 성능(RMSE)은 오히려 trainig set의 RMSE보다 낮게 나왔다.
```
- 모델은 특정한 세트에만 적절한(과적합) 것이 좋은 것이 아니라 새로운 세트에도 적용될 수 있는 모델이 좋은 것이다.
- 따라서 validation 성능(RMSE)이 좋다고 하여 꼭 test 성능(RMSE) 가 좋은 것은 아니다.
- 일반적인 경향성을 가질 뿐이다.

