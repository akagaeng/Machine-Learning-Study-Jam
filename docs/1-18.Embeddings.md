# 임베딩 (Embedding)

## 임베딩이란?

> 임베딩은 고차원 벡터의 변환을 통해 생성할 수 있는 상대적인 저차원 공간을 가리킵니다. 임베딩을 사용하면 단어를 나타내는 희소 벡터와 같이 커다란 입력값에 대해 머신러닝을 더 쉽게 수행할 수 있습니다. 임베딩이 잘 동작하는 경우 의미가 유사한 입력값들을 임베딩 공간 안에 서로 근접하게 위치시켜 입력값의 특정 의미를 포착합니다. 임베딩은 모델과 관계없이 학습과 재사용이 가능합니다.

### 협업 필터링에서 필요한 경우

> **협업필터링**은 다른 여러 사용자의 관심분야를 바탕으로 특정 사용자의 관심분야를 예측하는 작업

- 입력: 사용자 50만명이 선택한 영화 100만편
- 작업: 사용자에게 영화 추천

이 문제를 해결하려면 어떤 영화가 서로 비슷한지 파악해야 한다.

1. **1차원** 수직선에 유사성별로 영화 정렬

- 애니메이션 ---> 성인영화
```46500 + 8800 + 2500 = 11300 + 46500 = 57800
슈렉 - 인크레더블 - 벨빌의 세 쌍둥이 - 해리포터 - 스타워즈 - 세 가지 색: 블루 - 다크나이트 라이즈 - 메멘토
```
- 1차원으로 표현시 사람들이 어떤 요소들을 좋아하는지 복잡하게 파악하기는 어렵다.

2. **2차원** 공간에서 유사성별로 영화 정렬
  - x축: 어린이 ~ 성인
  - y축: 예술영화 ~ 블록버스터
  - 파악하고싶은 요소를 2가지(x축,y축) 고려할 수 있다.
  - 2차원 좌표로 표현되는 **임베딩 공간**에 영화들을 매핑
  - 슈렉(-1.0, 0.95) 블루(0.65, -0.2)로 표현할 수 있다.
  - 추천해줄 영화를 선정할 때 유사성이 있을수록 임베딩 공간에서의 영화 간의 거리가 가깝게 나타나므로 그 영화를 추천해줄 수 있음

### d차원 임베딩
- 영화에 대한 사용자의 관심분야를 대략 d개의 측면에서 설명할 수 있다고 가정
- 각각의 영화는 차원 d의 값이 해당 측면에 대한 각 영화의 일치도를 나타내는 d차원의 지점이 된다.
- 임베딩은 데이터를 통해 학습할 수 있다.

## 범주형 입력 데이터

### 범주형 데이터란(Categorical Data)?
- 선택사항이 유한한 집합에 속한 하나 이상의 이산 항목을 표현하는 입력 특성
- 대부분의 요소가 0인 희소 텐서를 통해 가장 효율적으로 표현된다.

### 원-핫 인코딩
- 슈렉을 색인(index) 0번에 할당한 후에 다음 망(network)에 '슈렉'을 전달하는 경우 0번 노드에는 1을, 나머지 노드에는 0을 입력한다.

```
[슈렉, 인크레더블, 벨빌의 세 쌍둥이, 해리포터, 스타워즈, 세 가지 색: 블루, 다크나이트 라이즈, 메멘토]

따라서 0번은 슈렉, 1번은 인크레더블, ...
```

어떤 사용자 x가 본 영화를 다음과 같이 표현할 수 있다.
```
(0, 1, 4)

이는 즉 슈렉, 인크레더블, 스타워즈를 의미한다.
```

### 망 크기
입력 벡터가 거대해지면 신경망에 엄청나게 많은 가중치가 만들어진다.
어휘에 M개의 단어가 있고 입력 위에 있는 망의 첫 번째 레이어에 N개의 노드가 있으면 해당 레이어에 대해 MxN 개의 가중치를 학습시켜야 한다. 가중치의 수가 커지면 다음과 같은 문제가 발생한다.
- 데이터의 양: 모델의 가중치가 많을수록 효과적인 학습을 위해 더 많은 데이터가 필요하다.
- 계산량: 가중치가 많을수록 모델을 학습하고 사용하는 데 더 많은 계산이 필요하다. 이를 위해서는 하드웨어가 뒷받침되어야 한다.

## 임베딩: 저차원 공간으로 변환
고차원 데이터를 저차원 공간에 매핑하여 희소한 입력데이터의 핵심 문제를 해결할 수 있다.

## 임베딩 획득하기
### 표준 차원 축소 기법
저차원 공간에서 고차원 공간의 중요한 구조를 캡쳐할 수 있는 수학적 기법이 여러 개 존재하는데, 이론상으로 이러한 기법들은 모두 시스템용 임베딩을 만드는 데 사용할 수 있다.
- Word2vec
  + 단어 임베딩 학습을 위해 Google에서 개발한 알고리즘
  + 분포 가설에 기반하여 의미론적으로 유사한 단어(문장 내에서 자주 함께하는 단어)를 기하학적으로 가까운 임베딩 벡터로 매핑한다.
  + 

### 심층망에서 임베딩 학습
- 임베딩 레이어는 차원당 단위 하나를 갖는 히든 레이어에 불과하므로 별도의 학습 과정이 필요하지 않다.
- 지도 정보(예: 사용자가 동일한 영화 2개를 시청함)를 통해 원하는 작업에 맞게 학습된 임베딩을 조정한다.
- 히든 단위는 최종 목표를 최적화하도록 d차원 공간에서 항목을 정리하는 방법을 직관적으로 발견한다.



### 임베딩 차원 개수 선택
- 고차원 임베딩은 입력값 간의 관계를 더 정확하게 표현할 수 있다.
- 하지만 차원이 많아지면 과적합 확률이 높아져 학습속도가 느려진다.
- 경험적 법칙은 시작점으로는 좋지만 유효성 확인 데이터를 사용하여 조정하여야 한다.
  + 경험적 법칙: dimensions ~= possible values^(1/4)



