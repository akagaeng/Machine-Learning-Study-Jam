## 일반화
### 과적합의 위험
과적합 모델은 학습하는 동안 손실이 적지만 새 데이터를 잘 예측하지 못한다.

머신러닝의 근본적인 과제는 데이터 적합도를 유지하는 동시에 최대한 단순화하는 것이다. (필요 이상으로 복잡한 모델을 만들면 과적합 발생)

Occam의 면도날 법칙
- 14세기의 수도사이자 철학자인 William of Occam
- 과학자는 복잡한 것보다 간단한 공식이나 이론을 선택해야 한다고 하였음
- => ML 모델이 덜 복잡할수록 샘플의 특성 때문이 아니어도 좋은 경험적 결과를 얻을 가능성이 높다!

일반화 한계(Generalization Bounds)
- Occam의 면도날 법칙을 통계적 학습이론 및 컴퓨터 학습 이론 분야에서 공식화한 것
- 모델의 복잡성, 학습 데이터에 대한 모델의 성능을 기반으로 새 데이터에 맞게 모델이 일반화되는 정도를 나타낸다.

일반화 판단
- 학습세트와 테스트 세트로 나누어 판단

### ML 세부사항
일반화에서 다음 세가지를 가정한다.
1. i.i.d
  - 분포에서 독립적이고, 동일하게 예를 추출한다고 가정
2. stationary
  - 데이터 세트 내에서 분포가 달라지지 않는다.
3. 같은 분포를 따르는 부분에서 예를 추출한다.
