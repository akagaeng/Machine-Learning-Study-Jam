# 정규화: 희소성

## 희소성을 위한 정규화: L1 정규화

### 희소 벡터의 문제
- 희소 벡터는 많은 차원을 포함하며, 특성교차를 생성하는 경우 더 많은 차원이 발생한다.
- 고차원 특성 벡터가 주어지면 모델의 크기가 커지며, 엄청난 양의 RAM이 필요하다.
- 가능한 경우 고차원 희소벡터는 가중치가 0이 되도록 하는 것이 좋음
  + 모델에서 해당 특성을 삭제할 수 있다.
  + RAM이 절약되고, 모델의 노이즈가 줄어들 수 있다.
  
### 희소 벡터 제거하는 방법: L1 정규화
- L2 정규화 (x)
  + 가중치를 작은 값으로 유도하기는 하나 정확히 0.0으로 만들지는 못한다.
- L0 정규화 (x)
  + 모델에서 0이 아닌 계수의 값의 수에 페널티를 주는 정규화 항을 생성
  + 볼록 최적화 문제를 NP-난해 문제로 바꾸어버리는 단점
  + 결국 가중치를 0.0으로 만들기에 효과적이지 않음
- L1 정규화 (o)
  + L0에 가까우면서도 계산하기 효율적

### L1 정규화와 L2 정규화 비교
- L2와 L1은 서로 다른 방식으로 가중치에 페널티를 줍니다.
  + L2는 `가중치^2`에 페널티를 줍니다.
  + L1은 `|가중치|`에 페널티를 줍니다.

> 결과적으로 L2와 L1은 서로 다르게 미분됩니다.

- L2의 미분계수는 `2 * 가중치`입니다.
  + 매번 가중치의 x%만큼을 제거한다고 생각하면 되는데, 이 과정을 계속 반복해도 값이 절대 0이 되지 않는다.
- L1의 미분계수는 `k(가중치와 무관한 값을 갖는 상수)`입니다.
  + 매번 가중치에서 일정 상수를 빼는 것으로 생각하면 되는데, 이로 인해 0이 될 수 있음

### L1 정규화 실험

[L1 정규화 실험](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization)

- 정규화 없거나 L2 정규화인 경우에는 가중치가 0.0이 나오지 않음 
- L1 정규화의 경우 가중치가 3개는 0.0 이 나옴

## 플레이그라운드 실습

### L1 정규화 검사

1. L2에서 L1 으로 정규화를 전환하면 테스트 손실과 학습 손실 사이의 델타에 어떤 영향을 주나요?
- 정규화율 0.1인 경우
  + L2 정규화의 델타: 0.09정도 
  + L1 정규화의 델타: 0.04정도 (감소)
- 정규화율 0.3인 경우
  + L2 정규화의 델타: 0.07정도 
  + L1 정규화의 델타: 0.02정도 (감소)

2. L2에서 L1으로 정규화를 전환하면 학습된 가중치에 어떤 영향을 주나요?
- 정규화율 0.1인 경우
  + L2 정규화에서 가중치(x1,x2,x1^2,x2^2,x1x2): (-0.25, -0.16, 0.0067, -0.022, 0.39)  
  + L1 정규화에서 가중치(x1,x2,x1^2,x2^2,x1x2): (-0.063, -0.063, 0.0, 0.0, 0.21)    => 감소(완화)
- 정규화율 0.3인 경우
  + L2 정규화에서 가중치(x1,x2,x1^2,x2^2,x1x2): (-0.16, -0.098, 0.0085, -0.024, 0.29) 
  + L1 정규화에서 가중치(x1,x2,x1^2,x2^2,x1x2): (0.0, 0.0, 0.0, -0.019, 0.19)    => 감소(완화)
3. L1 정규화율(람다)을 높이면 학습된 가중치에 어떤 영향을 주나요?
- L1 정규화에서 가중치(x1,x2,x1^2,x2^2,x1x2): (0.0, 0.0, 0.0, 0.0, 0.0)
  + L1 정규화율을 올리면 일반적으로 학습된 가중치가 완화(감소)된다.
  + 그러나 정규화율이 너무 높으면 모델이 수렴하지 않고, 손실이 매우 높아진다. (여기서는 손실 0.5)




  